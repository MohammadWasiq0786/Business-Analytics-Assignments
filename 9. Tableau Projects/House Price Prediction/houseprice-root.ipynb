{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from scipy.stats import norm\n",
    "from pyglmnet import GLM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "#%matplotlib qt5\n",
    "\n",
    "#define custom functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Distribution and skewness\n",
    "def distskew(dataset,feature):\n",
    "    fig = plt.figure()\n",
    "    sns.distplot(dataset[feature], fit=norm);\n",
    "    return(\"Skewness = \",skew(dataset[feature].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "def scatplot(a,b):\n",
    "    scatplotdata = pd.DataFrame({\"x\":a, \"y\":b})\n",
    "    scatplotdata.plot(x = \"x\", y = \"y\", kind = \"scatter\")\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "train = pd.read_csv(\"D:/acads/ds/kaggle/house-price/train.csv\")  #this is the initial X matrix\n",
    "test = pd.read_csv(\"D:/acads/ds/kaggle/house-price/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatplot(train[\"GrLivArea\"],train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distskew(train,\"ScreenPorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n",
    "train = train.drop(train[train['Id'] == 1299].index)\n",
    "train = train.drop(train[train['Id'] == 524].index)\n",
    "\n",
    "#train = train.drop(train[train['GrLivArea'] > 4000].index)\n",
    "#train = train.drop(train[train['TotalBsmtSF'] == 0].index)\n",
    "\n",
    "train = train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatplot(train[\"GrLivArea\"],train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "#train.loc --> loc works by looking the labels of the index\n",
    "#the above operations then will extract all the data between \n",
    "#the column 'MSSubClass' and 'SaleCondition' both  included\n",
    "\n",
    "#all_data.head() #as you can see the ID and SalePrice column disappeared \n",
    "                     \n",
    "                     \n",
    "                     \n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\n",
    "#prices.hist()\n",
    "\n",
    "#log transform the target:\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "#here we can list all the data by each types\n",
    "#data_by_types = all_data.columns.to_series().groupby(all_data.dtypes).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[\"TotBsmtFin\"] = all_data[\"BsmtFinSF1\"] + all_data[\"BsmtFinSF2\"]\n",
    "train[\"TotBsmtFin\"] = train[\"BsmtFinSF1\"] + train[\"BsmtFinSF2\"]\n",
    "                 \n",
    "all_data = all_data.drop(\"BsmtFinSF1\",1)\n",
    "all_data = all_data.drop(\"BsmtFinSF2\",1)\n",
    "\n",
    "all_data[\"TotBath\"] = all_data[\"FullBath\"] + 0.5*all_data[\"HalfBath\"] + all_data[\"BsmtFullBath\"] + 0.5*all_data[\"BsmtHalfBath\"]\n",
    "train[\"TotBath\"] = train[\"FullBath\"] + 0.5*train[\"HalfBath\"] + train[\"BsmtFullBath\"] + 0.5*train[\"BsmtHalfBath\"]\n",
    "\n",
    "all_data = all_data.drop(\"FullBath\",1)\n",
    "all_data = all_data.drop(\"HalfBath\",1)\n",
    "all_data = all_data.drop(\"BsmtFullBath\",1)\n",
    "all_data = all_data.drop(\"BsmtHalfBath\",1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new feature to handle the zero values of TotalBsmtSF\n",
    "#all_data[\"ZeroBsmt\"] = 0\n",
    "#all_data.loc[all_data[\"TotalBsmtSF\"] == 0,\"ZeroBsmt\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#avgTotalBsmtSF = all_data[\"TotalBsmtSF\"].mean()\n",
    "#all_data.loc[all_data[\"TotalBsmtSF\"] == 0,\"TotalBsmtSF\"] = avgTotalBsmtSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the living areas and basement aread to create a new feature TotArea\n",
    "all_data[\"TotArea\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n",
    "train[\"TotArea\"] = train[\"GrLivArea\"] + train[\"TotalBsmtSF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take log of the skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the index of the numerical features\n",
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "\n",
    "\n",
    "#for all the numeric features, calculate the skewness\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "\n",
    "#exctract the features with skewness higher than 75%\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.1]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "#log transform skewed numeric features with skewness > 75%:\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "train[skewed_feats] = np.log1p(train[skewed_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop some features that do not help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = all_data.drop(\"BsmtFinType1\",1)\n",
    "all_data = all_data.drop(\"2ndFlrSF\",1)\n",
    "all_data = all_data.drop(\"BedroomAbvGr\",1)\n",
    "\n",
    "all_data = all_data.drop(\"LowQualFinSF\",1)\n",
    "all_data = all_data.drop(\"3SsnPorch\",1)\n",
    "#all_data = all_data.drop(\"PoolArea\",1)\n",
    "\n",
    "#all_data = all_data.drop(\"GrLivArea\",1)\n",
    "#all_data = all_data.drop(\"TotalBsmtSF\",1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(all_data)\n",
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "#pd.get_dummies(all_data)\n",
    "\n",
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())\n",
    "\n",
    "# Normalize the features (this does not seem to help: increases error)\n",
    "#all_data = all_data.apply(lambda x: x/np.sqrt(sum(x**2)))\n",
    "\n",
    "#creating matrices for sklearn:\n",
    "X_train = all_data[:train.shape[0]]\n",
    "X_test = all_data[train.shape[0]:]\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distskew(all_data,\"ScreenPorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatplot(train[\"GrLivArea\"],train[\"SalePrice\"])\n",
    "scatplot(train[\"TotalBsmtSF\"],train[\"SalePrice\"])\n",
    "scatplot(train[\"TotArea\"],train[\"SalePrice\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model: Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first evaluation of the regular linear regression method:\n",
    "#Ridge:\n",
    "model_ridge = Ridge()\n",
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n",
    "           for alpha in alphas]\n",
    "    \n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(title = \"Validation - Just Do It\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.show()\n",
    "print(\"The min value of Ridge is \",cv_ridge.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now fit Ridge model\n",
    "model_ridge = Ridge(alpha = 5).fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distskew(X_train,\"GrLivArea\")\n",
    "distskew(X_train,\"TotalBsmtSF\")\n",
    "distskew(X_train,\"TotArea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lasso:\n",
    "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n",
    "print(\"The min value of Lasso is \",rmse_cv(model_lasso).mean())\n",
    "\n",
    "\n",
    "#Lasso choose performs also feature selection \n",
    "\n",
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "#Let's look to the most important coefficients:\n",
    "imp_coef = pd.concat([coef.sort_values().head(10),\n",
    "                     coef.sort_values().tail(10)])\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")  \n",
    "plt.show()  \n",
    "\n",
    "#Also note that unlike the feature importance you'd get from a random forest \n",
    "#these are actual coefficients in your model - so you can say precisely \n",
    "#why the predicted price is what it is. \n",
    "#The only issue here is that we log_transformed both the target \n",
    "#and the numeric features \n",
    "#so the actual magnitudes are a bit hard to interpret.\n",
    "\n",
    "\n",
    "#let's look at the residuals as well:\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "\n",
    "preds_log = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\n",
    "#let's return to \"original\" value from the log:\n",
    "preds_val = np.expm1(preds_log)\n",
    "\n",
    "#solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds_val})\n",
    "#solution.to_csv(\"MDV_Lasso_v1.csv\", index = False)\n",
    "\n",
    "\n",
    "preds_log[\"residuals\"] = preds_log[\"true\"] - preds_log[\"preds\"]\n",
    "\n",
    "\n",
    "\n",
    "#preds_log.plot(x = \"preds\", y = \"true\",kind = \"scatter\")\n",
    "#plt.title('Log Value ')\n",
    "#plt.show()\n",
    "\n",
    "#\n",
    "#preds_val[\"residuals\"] = preds_val[\"true\"] - preds_val[\"preds\"]\n",
    "#preds_val.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\n",
    "#plt.title('Original Value in $')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's add an xgboost model to our linear model to see \n",
    "#if we can improve our score:\n",
    "\n",
    "#y = preds_log[\"residuals\"]\n",
    "    \n",
    "dtrain = xgb.DMatrix(X_train, label = y)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "params = {\"max_depth\":2, \"eta\":0.1}\n",
    "model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n",
    "\n",
    "model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n",
    "plt.show()\n",
    "\n",
    "#the params were tuned using xgb.cv\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=300, max_depth=2, learning_rate=0.1) \n",
    "model_xgb.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now fit GLM net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_glm = GLM(distr='gaussian')\n",
    "#scaler1 = StandardScaler().fit(X_train)\n",
    "#scaler2 = StandardScaler().fit(X_test)\n",
    "#model_glm.fit(scaler1.transform(X_train), y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now combine the models for final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "ridge_preds = np.expm1(model_ridge.predict(X_test))\n",
    "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
    "glm_preds = np.expm1(model_glm.predict(scaler2.transform(X_test)))\n",
    "\n",
    "predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\n",
    "predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")\n",
    "plt.show()\n",
    "\n",
    "#Many times it makes sense to take a weighted average of uncorrelated results - this usually imporoves \n",
    "#the score although in this case it doesn't help that much.\n",
    "\n",
    "preds = 0.30*lasso_preds + 0.30*lasso_preds + 0.40*xgb_preds\n",
    "\n",
    "solution = pd.DataFrame({\"SalePrice\":preds,\"id\":test.Id})\n",
    "\n",
    "solution.to_csv(\"D:/acads/ds/kaggle/house-price/my_solution_eight.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(model_lasso.predict(X_test), model_ridge.predict(scaler2.transform(X_test)), 'o', label='original data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 0.30*model_lasso.predict(X_train) + 0.30*model_ridge.predict(X_train) + 0.40*model_xgb.predict(X_train)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, intercept + slope*x, 'r', label='fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"r-squared:\", r_value**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
